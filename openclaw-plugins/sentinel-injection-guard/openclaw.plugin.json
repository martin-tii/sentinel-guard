{
  "id": "sentinel-injection-guard",
  "name": "Sentinel Injection Guard",
  "description": "Prompt-injection and jailbreak detection using Llama Prompt Guard + Llama Guard, with strict-mode enforcement.",
  "configSchema": {
    "type": "object",
    "additionalProperties": false,
    "properties": {
      "enabled": { "type": "boolean" },
      "ollamaEndpoint": { "type": "string" },
      "failMode": { "type": "string", "enum": ["open", "closed"] },
      "promptGuardModel": { "type": "string" },
      "promptGuardThreshold": { "type": "number" },
      "promptGuardMaxLength": { "type": "number" },
      "promptGuardFailOpen": { "type": "boolean" },
      "promptGuardBridgeEnabled": { "type": "boolean" },
      "promptGuardBridgeScript": { "type": "string" },
      "promptGuardBridgePython": { "type": "string" },
      "promptGuardBridgeSource": { "type": "string" },
      "llamaGuardModel": { "type": "string" },
      "strictTools": {
        "type": "array",
        "items": { "type": "string" }
      },
      "riskyTools": {
        "type": "array",
        "items": { "type": "string" }
      }
    }
  }
}
